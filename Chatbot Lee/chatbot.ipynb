{"cells":[{"cell_type":"markdown","metadata":{"id":"SfaovCyGb4y1"},"source":["# Library Installation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_EFb5ZZCrwi5"},"outputs":[],"source":["!pip install numpy\n","!pip install nltk\n","!pip install keras tensorflow\n","!pip install Flask Flask-ngrok\n","!pip install chatterbot chatterbot-corpus\n","!pip install -U scikit-fuzzy\n","!pip install matplotlib"]},{"cell_type":"markdown","metadata":{"id":"jriMWnQ5b-7i"},"source":["# Weather Data ðŸ“ˆ"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ezfzSH3drwjL"},"outputs":[],"source":["%run weather_scrap.ipynb"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EhgnGuSurwjM"},"outputs":[],"source":["%run fuzzy.ipynb"]},{"cell_type":"markdown","metadata":{"id":"kVWxrwR7bxub"},"source":["# Training Model Section"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0D3a2ahqrwjN"},"outputs":[],"source":["import random\n","import json\n","import pickle\n","import numpy as np\n","import nltk\n","nltk.download('punkt')\n","nltk.download('wordnet')\n","from nltk.stem import WordNetLemmatizer\n","from keras.models import Sequential\n","from keras.layers import Dense, Activation, Dropout\n","from tensorflow.keras.optimizers import SGD\n","\n","# Constructor\n","lemmatizer = WordNetLemmatizer()\n","\n","# Load & Read intents Json file\n","intents = json.loads(open('intents.json').read())\n","\n","# Entries List for word Part ---\n","words = []\n","classes = []\n","documents = []\n","ignore_letters = ['?','!','.',',']  # ignore those letter\n","\n","# Tokenisation\n","for intent in intents['intents']:\n","    for pattern in intent['patterns']:\n","\n","        # Take Each Word and Tokenize it\n","        word_list = nltk.word_tokenize(pattern)\n","        words.extend(word_list)\n","        documents.append((word_list, intent['tag']))\n","\n","        # Check the classes already in the document\n","        if intent['tag'] not in classes:\n","            classes.append(intent['tag'])\n","\n","# Lemmatizer\n","words = [lemmatizer.lemmatize(word) for word in words if word not in ignore_letters]\n","words = sorted(set(words))\n","classes = sorted(set(classes))\n","\n","pickle.dump(words, open('words.pkl', 'wb'))\n","pickle.dump(classes, open('classes.pkl', 'wb'))\n","\n","# Training Data Part ---\n","training = []\n","output_empty = [0] * len(classes)\n","\n","# Generate Bag of Words\n","for document in documents:\n","    bag = []\n","    # List of Tokenized words for the pattern\n","    word_patterns = document[0]\n","    word_patterns = [lemmatizer.lemmatize(word.lower()) for word in word_patterns]\n","\n","    # Create Bag of words array with 1, if word match found in current pattern\n","    for word in words:\n","        bag.append(1) if word in word_patterns else bag.append(0)\n","\n","    # Output From List\n","    output_row = list(output_empty)\n","    # Save the index 1 from the document into output\n","    output_row[classes.index(document[1])] = 1\n","    training.append([bag, output_row])\n","\n","# Shuffle the Training Data and Put into np.array\n","random.shuffle(training)\n","training = np.array(training)\n","\n","# Create Train and Test lists. X - patterns, Y - intents\n","train_x = list(training[:, 0])\n","train_y = list(training[:, 1])\n","print(\"Training data created\")\n","\n","# Neural Network\n","# Create Training Model Part ---\n","model = Sequential()\n","model.add(Dense(128, input_shape=(len(train_x[0]),), activation='relu'))\n","model.add(Dropout(0.5))\n","model.add(Dense(64, activation='relu'))\n","model.add(Dropout(0.5))\n","model.add(Dense(len(train_y[0]), activation='softmax'))\n","\n","sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n","model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n","\n","# Fitting and Saving the model\n","hist = model.fit(np.array(train_x),np.array(train_y), epochs=200, batch_size=5, verbose=1)\n","model.save('chatbot_model.h5', hist)\n","print(\"Model Created\")"]},{"cell_type":"markdown","metadata":{"id":"YDykBloXbq2q"},"source":["# Chatbot Section ðŸ’»\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5gFL-h_xrwjQ"},"outputs":[],"source":["import random\n","import json\n","import pickle\n","import numpy as np\n","\n","from flask import Flask, render_template, request\n","from flask_ngrok import run_with_ngrok\n","\n","import nltk\n","from nltk.stem import WordNetLemmatizer\n","from keras.models import load_model\n","\n","import re\n","\n","# Constructor\n","lemmatizer = WordNetLemmatizer()\n","\n","# Load & Read intents Json file\n","intents = json.loads(open('intents.json').read())\n","weather_intents = json.loads(open('weather_intents.json').read())\n","\n","# Load the Pickle file & Model file\n","words = pickle.load(open('words.pkl', 'rb'))\n","classes = pickle.load(open('classes.pkl', 'rb'))\n","model = load_model('chatbot_model.h5')\n","\n","app = Flask(__name__)\n","run_with_ngrok(app)\n","\n","month_list = [\n","\"january\",\n","\"february\",\n","\"march\",\n","\"april\",\n","\"may\",\n","\"june\",\n","\"july\",\n","\"august\",\n","\"september\",\n","\"october\",\n","\"november\",\n","\"december\"\n","]\n","\n","@app.route(\"/\")\n","def home():\n","    return render_template(\"index.html\")\n","\n","@app.route(\"/get\", methods=[\"POST\"])\n","def chatbot_response():\n","    # Get message from user input\n","    msg = request.form[\"msg\"]\n","    # Convert the sentence to lower case\n","    lower_msg = msg.lower()\n","    # Remove special characters\n","    filter_msg = re.sub('[^A-Za-z ]+', '', lower_msg)\n","    filter_msg2 = re.sub('[^A-Za-z ]+', '', msg)\n","    # Spilt words in a sentence\n","    spilt_msg = [word for word in filter_msg.split()]\n","\n","    # Check boolean state for matching results in weather_intents and month_list (destination, months)\n","    found = False\n","\n","    for msg_word in spilt_msg:\n","        intents_list = weather_intents['intent_dict']\n","\n","        # Loop through weather_intents\n","        for i in intents_list:\n","            # Check destination (cities name) and convert to lower case\n","            check = i['destination'].lower()\n","            # Spilt cities name\n","            spilt_check = [word for word in check.split()]\n","\n","            # Assign empty string for the variable\n","            final_destination_name = \"\"\n","\n","            if msg_word in spilt_check:\n","                # Get current index of the word in the string array\n","                current_index = spilt_msg.index(msg_word)\n","\n","                if msg_word == i['destination'].lower():\n","                    # For city name that has only 1 word\n","                    destination_name = str(spilt_msg[current_index])\n","                else:\n","                    # For city name with more than 1 word\n","                    destination_name = str(spilt_msg[current_index - 1]) + \" \" + str(spilt_msg[current_index])\n","\n","                # Assign the combined words for a city name to final_destination_name\n","                final_destination_name = destination_name\n","\n","            # If the city name matched with the value in the intents\n","            if final_destination_name == i['destination'].lower():\n","                lower_destination_str = i['destination'].lower()\n","\n","                for msg_word in spilt_msg:\n","                    #If month is found in the month_list\n","                    if msg_word in month_list:\n","                        # Used to check if message from user contains both city's name and month\n","                        found = True\n","\n","                        # Information to be displayed\n","                        prob = i['months'][msg_word]['probability']\n","                        avg_temp_f = i['months'][msg_word]['avg_temp']\n","                        avg_temp_c = (avg_temp_f - 32) * 5/9\n","                        avg_temp_c = \"{:.2f}\".format(avg_temp_c)\n","                        clearer = i['months'][msg_word]['clearer']\n","                        precipitation = i['months'][msg_word]['precipitation']\n","                        rainfall = i['months'][msg_word]['rainfall']\n","                        windspeed = i['months'][msg_word]['windspeed']\n","\n","                        # If probability more than 50\n","                        if(prob > 50):\n","                            res = \"<b><u>\" + \"Average Weather information:\" + \"</u></b>\" + \"<br>\" + \"Temperature: \" + str(avg_temp_c) + \"Â°C\" + \"<br>\" + \"Clear days: \" + str(clearer) + \"%\" + \"<br>\" + \"Precipitation: \" + str(precipitation) + \"d\" + \"<br>\" + \"Rainfall: \" + str(rainfall) + \"â€³\" + \"<br>\" + \"Windspeed (mph): \" + str(windspeed) + \"<br>\" + \"<br>\" + \"Good option! \" + lower_destination_str.title() + \" is suitable to go in \" + msg_word.capitalize() + \".ðŸ‘\"\n","                        else:\n","                            # Find months that has probability more than 50\n","                            destination_months = i['months']\n","                            all_keys = []\n","                            for key, value in destination_months.items():\n","                                if(value['probability'] > 50):\n","                                    all_keys.append(key.capitalize())\n","\n","                            # Output message part 1 (All weather information for that particular city on particular month)\n","                            res1 = \"<b><u>\" + \"Average weather information:\" + \"</u></b>\" + \"<br>\" + \"Temperature: \" + str(avg_temp_c) + \"Â°C\" + \"<br>\" + \"Clear days: \" + str(clearer) + \"%\" + \"<br>\" + \"Precipitation: \" + str(precipitation) + \"d\" + \"<br>\" + \"Rainfall: \" + str(rainfall) + \"â€³\" + \"<br>\" + \"Windspeed (mph): \" + str(windspeed) + \"<br>\" + \"<br>\" + \"Hmm... I would not recommend you to go \" + lower_destination_str.title() + \" in \" + msg_word.capitalize() + \".ðŸ˜«\"\n","                            # Output message part 2 (For suitable months to visit)\n","                            res2 = ', '.join(all_keys)\n","                            res = res1 + \" <br><br> Perhaps, you may choose to visit \" + lower_destination_str.title() + \" in \" + res2 + \".\"\n","\n","    # When both (city name AND month) is not found, get response through intent.json\n","    if found == False:\n","        if filter_msg2.startswith(\"my name is\"):\n","            name = filter_msg2[11:]\n","            ints = predict_class(filter_msg2, model)\n","            res1 = get_response(ints, intents)\n","            res = res1.replace(\"{n}\", name)\n","        elif filter_msg2.startswith(\"hi my name is\"):\n","            name = filter_msg2[14:]\n","            ints = predict_class(filter_msg2, model)\n","            res1 = get_response(ints, intents)\n","            res = res1.replace(\"{n}\", name)\n","        elif filter_msg2.startswith(\"i am\"):\n","            name = filter_msg2[5:]\n","            ints = predict_class(filter_msg2, model)\n","            res1 = get_response(ints, intents)\n","            res = res1.replace(\"{n}\", name)\n","        else:\n","            ints = predict_class(msg, model)\n","            res = get_response(ints, intents)\n","\n","    return res\n","\n","# Function : Clearing sentences\n","def clean_up_sentence(sentence):\n","    sentence_words = nltk.word_tokenize(sentence)\n","    sentence_words = [lemmatizer.lemmatize(word.lower()) for word in sentence_words]\n","    return sentence_words\n","\n","# Function : Bag\n","def bag_of_words(sentence, words, show_details=True):\n","    sentence_words = clean_up_sentence(sentence)\n","    bag = [0] * len(words)\n","    for w in sentence_words:\n","        for i, word in enumerate(words):\n","            if word == w:\n","                bag[i] = 1\n","            if show_details:\n","                    print(\"found in bag: %s\" % w)\n","    return np.array(bag)\n","\n","# Function : Predict the sentences\n","def predict_class(sentence, model):\n","    #bow = bag_of_words(sentence)\n","    bow = bag_of_words(sentence, words, show_details=False)\n","    res = model.predict(np.array([bow]))[0]\n","    ERROR_THRESHOLD = 0.25      # 25%\n","    results =[[i, r] for i, r in enumerate(res) if r > ERROR_THRESHOLD]\n","\n","    results.sort(key=lambda x: x[1], reverse=True)\n","    return_list = []\n","    for r in results:\n","        return_list.append({'intent': classes[r[0]], 'probability': str(r[1])})\n","    return return_list\n","\n","# Function : Response\n","def get_response(intents_list, intents_json):\n","    tag = intents_list[0]['intent']\n","    list_of_intents = intents_json['intents']\n","    for i in list_of_intents:\n","        if i['tag'] == tag:\n","            result = random.choice(i['responses'])\n","            break\n","    return result\n","\n","if __name__ == \"__main__\":\n","    app.run()\n"]}],"metadata":{"colab":{"collapsed_sections":[],"name":"chatbot.ipynb","provenance":[]},"interpreter":{"hash":"fcea7bec55ff6d7f3b1107f41f074eaa5711bec3d5478eb754b653bcca62ae21"},"kernelspec":{"display_name":"Python 3.9.8 64-bit (windows store)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"}},"nbformat":4,"nbformat_minor":0}
